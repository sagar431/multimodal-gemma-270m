# Multimodal Gemma-270M MLOps Project

[![Train & Deploy](https://github.com/YOUR_USERNAME/multimodal-gemma-270m/actions/workflows/train_deploy.yml/badge.svg)](https://github.com/YOUR_USERNAME/multimodal-gemma-270m/actions/workflows/train_deploy.yml)
[![HuggingFace Space](https://img.shields.io/badge/ğŸ¤—-HuggingFace%20Space-yellow)](https://huggingface.co/spaces/YOUR_USERNAME/multimodal-gemma-270m)

A production-ready **Multimodal Vision-Language Model** built with PyTorch Lightning and automated MLOps CI/CD pipeline for deployment to HuggingFace Spaces.

## ğŸŒŸ Features

- **Multimodal Architecture**: Combines Google Gemma-270M with CLIP vision encoder
- **PyTorch Lightning**: Clean, modular training code with automatic optimization
- **MLOps Pipeline**: Automated CI/CD with GitHub Actions
- **DVC Integration**: Data versioning and pipeline orchestration
- **Auto-Deployment**: Push to main â†’ Test â†’ Train â†’ Deploy to HuggingFace Spaces
- **Gradio Interface**: Beautiful, interactive web UI for inference

## ğŸ“ Project Structure

```
multimodal-gemma-270m/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ train_deploy.yml    # CI/CD pipeline
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ config.yaml            # Main Hydra config
â”‚   â”œâ”€â”€ model_config.yaml      # Model architecture
â”‚   â”œâ”€â”€ training_config.yaml   # Training hyperparameters
â”‚   â””â”€â”€ data_config.yaml       # Dataset configuration
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ lightning_module.py   # PyTorch Lightning module
â”‚   â”‚   â”œâ”€â”€ multimodal_gemma.py   # Core model architecture
â”‚   â”‚   â””â”€â”€ projectors.py         # Vision/Audio projectors
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ datamodule.py         # Lightning DataModule
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ config.py             # Configuration utilities
â”‚   â””â”€â”€ trace_model.py            # Model export for deployment
â”œâ”€â”€ hf_space/
â”‚   â”œâ”€â”€ app.py                 # Gradio app for HuggingFace Spaces
â”‚   â”œâ”€â”€ requirements.txt       # Space dependencies
â”‚   â””â”€â”€ README.md              # Space metadata
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ prepare_data.py        # Data preparation
â”‚   â””â”€â”€ validate_model.py      # Model validation
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_model.py          # Model unit tests
â”‚   â””â”€â”€ test_app.py            # App tests
â”œâ”€â”€ train.py                   # Main training script
â”œâ”€â”€ gradio_app.py              # Local Gradio app
â”œâ”€â”€ dvc.yaml                   # DVC pipeline definition
â”œâ”€â”€ pyproject.toml             # Project configuration
â”œâ”€â”€ Dockerfile                 # Container definition
â”œâ”€â”€ Makefile                   # Convenience commands
â””â”€â”€ README.md                  # This file
```

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/YOUR_USERNAME/multimodal-gemma-270m.git
cd multimodal-gemma-270m

# Install dependencies
pip install -r requirements.txt

# Or use make
make install
```

### Training

```bash
# Full training
python train.py

# Quick test run
python train.py trainer.fast_dev_run=true

# With Weights & Biases logging
python train.py logging.use_wandb=true

# Or use make
make train
make train-fast
```

### Model Export & Deployment

```bash
# Export model for deployment
python src/trace_model.py --output_path hf_space/model.pt

# Validate exported model
python scripts/validate_model.py --model_path hf_space/model.pt

# Or use make
make trace
make validate
```

### Local Inference

```bash
# Run Gradio locally
cd hf_space && python app.py

# Or use the full local app
python gradio_app.py
```

## ğŸ”„ CI/CD Pipeline

The project uses GitHub Actions for automated MLOps:

```
Push to main â†’ Tests â†’ Train (optional) â†’ Trace â†’ Deploy to HuggingFace Spaces
```

### Pipeline Jobs

1. **Test**: Runs linting and unit tests
2. **Train**: Trains the model (manual trigger or on demand)
3. **Trace**: Exports model to deployment format
4. **Deploy**: Uploads to HuggingFace Spaces
5. **Integration Test**: Verifies deployed space

### GitHub Secrets Required

Set these in your repository settings:

| Secret | Description |
|--------|-------------|
| `HF_TOKEN` | HuggingFace API token with write access |
| `HF_USERNAME` | Your HuggingFace username |
| `WANDB_API_KEY` | (Optional) Weights & Biases API key |

### Manual Deployment

Trigger a deployment manually:

```bash
# Via GitHub CLI
gh workflow run train_deploy.yml

# With training
gh workflow run train_deploy.yml -f run_training=true -f max_epochs=5
```

## ğŸ“Š DVC Pipeline

Use DVC for reproducible ML pipelines:

```bash
# Run full pipeline
dvc repro

# Run specific stage
dvc repro train

# Visualize pipeline
dvc dag
```

### Pipeline Stages

```
prepare_data â†’ train â†’ trace â†’ validate
```

## ğŸ—ï¸ Architecture

### Model Components

- **Language Model**: Google Gemma-270M with LoRA adapters
- **Vision Encoder**: CLIP ViT-Large/14
- **Vision Projector**: MLP connecting vision to language
- **Training**: LLaVA-style multimodal instruction tuning

### Key Parameters

| Component | Size |
|-----------|------|
| Language Model | 270M parameters |
| Vision Encoder | 428M parameters |
| Trainable (LoRA + Projector) | ~18.6M parameters |

## ğŸ“ Configuration

Modify configs via Hydra:

```bash
# Change model
python train.py model.gemma_model_name=google/gemma-2b

# Change training
python train.py training.max_epochs=10 training.projector_lr=1e-4

# Use different experiment
python train.py experiment=my_experiment
```

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/ -v

# With coverage
pytest tests/ -v --cov=src --cov-report=html

# Or use make
make test
make test-cov
```

## ğŸ³ Docker

```bash
# Build image
docker build -t multimodal-gemma:latest .

# Train with GPU
docker run --gpus all -v $(pwd)/models:/app/models multimodal-gemma:latest

# Interactive shell
docker run --gpus all -it multimodal-gemma:latest bash
```

## ğŸ“š References

- [LLaVA Paper](https://arxiv.org/abs/2304.08485)
- [Gemma Technical Report](https://arxiv.org/abs/2403.08295)
- [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/)
- [HuggingFace Spaces](https://huggingface.co/docs/hub/spaces)

## ğŸ“„ License

Apache 2.0

## ğŸ™ Acknowledgments

- Google for Gemma models
- OpenAI for CLIP
- LLaVA team for multimodal architecture inspiration
- PyTorch Lightning team for the training framework